{
    "name": "tokenize",
    "commit": "6ee3ca75e6b856eeeedcaac195fc75258040737b - stuff",
    "tests": [
        "469 - single long",
        "471 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "6ee3ca75e6b856eeeedcaac195fc75258040737b - stuff",
    "tests": [
        "458 - single long",
        "468 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "6ee3ca75e6b856eeeedcaac195fc75258040737b - stuff",
    "tests": [
        "460 - single long",
        "489 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "6ee3ca75e6b856eeeedcaac195fc75258040737b - stuff",
    "tests": [
        "20 - single long",
        "24 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "dbad4384c25f88331a833b4e65e1114a3bfbdb6e - experimenting more with benchmarks",
    "tests": [
        "19 - single long",
        "23 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "dbad4384c25f88331a833b4e65e1114a3bfbdb6e - experimenting more with benchmarks",
    "tests": [
        "8 - single long",
        "10 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "dbad4384c25f88331a833b4e65e1114a3bfbdb6e - experimenting more with benchmarks",
    "tests": [
        "3 - single long",
        "7 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "63595f164cc2eeb5086438dac81debb114a2ff79 - splitting the files for compilation can be much fater",
    "tests": [
        "2 - single long",
        "3 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "63595f164cc2eeb5086438dac81debb114a2ff79 - splitting the files for compilation can be much fater",
    "tests": [
        "2 - single long",
        "3 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "5d655821a2f3c6f7d14c631a1b3162202fda7664 - splitting large files before parsing could be smart",
    "tests": [
        "2 - single long",
        "3 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "5d655821a2f3c6f7d14c631a1b3162202fda7664 - splitting large files before parsing could be smart",
    "tests": [
        "8 - full",
        "8 - parts",
        "2 - single long",
        "2 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "5d655821a2f3c6f7d14c631a1b3162202fda7664 - splitting large files before parsing could be smart",
    "tests": [
        "8 - full",
        "8 - parts",
        "8 - chunks",
        "2 - single long",
        "2 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "5d655821a2f3c6f7d14c631a1b3162202fda7664 - splitting large files before parsing could be smart",
    "tests": [
        "8 - full",
        "8 - parts",
        "7 - chunks",
        "2 - single long",
        "2 - split"
    ]
}
{
    "name": "tokenize",
    "commit": "5f28bddc0a67dcb71cef178746016d8679ac091e - stuff",
    "tests": [
        "74 - lex",
        "285 - rewrite",
        "54 - tokenize",
        "27 - parse",
        "18 - compile"
    ]
}
{
    "name": "tokenize",
    "commit": "5f28bddc0a67dcb71cef178746016d8679ac091e - stuff",
    "tests": [
        "73 - lex",
        "281 - rewrite",
        "56 - tokenize",
        "27 - parse",
        "18 - compile"
    ]
}
{
    "name": "tokenize",
    "commit": "5f28bddc0a67dcb71cef178746016d8679ac091e - stuff",
    "tests": [
        "11 - lex",
        "46 - rewrite",
        "9 - tokenize",
        "5 - parse",
        "3 - compile"
    ]
}
